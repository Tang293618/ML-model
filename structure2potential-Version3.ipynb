{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原子体系建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  #3D plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经验势$E_{\\mathrm{emp}}=V(r_1,r_2)$和切割半径$R_{\\mathrm{cut}}$表示函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emperical_potential(r1,r2,a=5,b=10):\n",
    "    d = distance.euclidean(r1,r2)\n",
    "    return a / (d ** 12) - b / (d ** 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_func(r_ij,r_c = 3.0):  #f_c(R_ij)\n",
    "    if r_ij > r_c:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0.5*(np.cos(np.pi * r_ij / r_c) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原子类,包含原子坐标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atom():\n",
    "    def __init__(self,coords):\n",
    "        self.coords = coords\n",
    "        self.individual_potential = 0\n",
    "        self.g1_value = 0  #径向对称函数\n",
    "        self.g2_value = 0  #径向对称函数\n",
    "        self.g3_value = 0  #径向对称函数\n",
    "    def individual_potential_cal(self,neighbors):\n",
    "        for neighbor in neighbors:  #neighbors为相对距离列表\n",
    "            self.individual_potential += 0.5 * emperical_potential(0,neighbor)\n",
    "    def g1_cal(self,neighbors):\n",
    "        for neighbor in neighbors:\n",
    "            self.g1_value += cutoff_func(neighbor)\n",
    "    def g2_cal(self,neighbors,eta = 0.1,r_s = 0.8):\n",
    "        for neighbor in neighbors:\n",
    "            self.g2_value += np.exp(-eta * (neighbor-r_s)**2)*cutoff_func(neighbor)\n",
    "    def g3_cal(self,neighbors,kappa = 0.4):\n",
    "        for neighbor in neighbors:\n",
    "            self.g3_value += np.cos(kappa*neighbor)*cutoff_func(neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原子点集类,含维数、总势能、原子个数、坐标范围等参数,并有随机生成、输入列表生成、势能计算等方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomCollection():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        coords_range: suppose (x, ...) is the coordinate of an atom while range is (a,b), then a<x<b, ...\n",
    "        \"\"\"\n",
    "        self.atoms = []\n",
    "        self.num = 0\n",
    "        self.dimension = 0\n",
    "        self.potential = 0\n",
    "        self.coords_range=(-5,5)\n",
    "        \n",
    "    def random_generate(self,size,dimension=2):\n",
    "        \"\"\"generate a collection of atoms by using random numbers\n",
    "        \n",
    "        Args:\n",
    "            size: the number of atoms in self.atoms\n",
    "            dimension: 2 or 3\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        temp_coords = np.random.uniform(self.coords_range[0],self.coords_range[1],\n",
    "                                        dimension).tolist()  #1*dimension shape-like list\n",
    "        self.num += 1\n",
    "        self.atoms.append(Atom(temp_coords))\n",
    "        while self.num < size:\n",
    "            flag = True\n",
    "            temp_coords = np.random.uniform(self.coords_range[0],self.coords_range[1],\n",
    "                                            dimension).tolist()\n",
    "            for atom in self.atoms:\n",
    "                if distance.euclidean(atom.coords,temp_coords) < 0.1:  #太近了先不考虑\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag is True:\n",
    "                self.num +=1\n",
    "                self.atoms.append(Atom(temp_coords))\n",
    "                    \n",
    "    def list_generate(self,coords_list):\n",
    "        \"\"\"generate a collection of atoms by using a given list\n",
    "        \n",
    "        Args:\n",
    "            coords_list: shape like [[1,1,...],[2,2,...],...]\n",
    "        \"\"\"\n",
    "        if coords_list == []:\n",
    "            return\n",
    "        for coords in coords_list:\n",
    "            self.atoms.append(Atom(coords))\n",
    "        self.num = len(coords_list)\n",
    "        self.dimension = len(coords_list[0])\n",
    "    \n",
    "    def distance_cal(self,center):  #计算体系中其他所有原子和中心原子的相对距离\n",
    "        dis = []\n",
    "        for atom in self.atoms:\n",
    "            if atom is not center:\n",
    "                dis.append(distance.euclidean(atom.coords,center.coords))\n",
    "        return dis\n",
    "    \n",
    "    def potential_cal(self):\n",
    "        \"\"\"calculate the total potential of the atom collection\"\"\"        \n",
    "        for atom in self.atoms: #将每个原子的势能累加得到体系总势能\n",
    "            self.potential += atom.individual_potential\n",
    "        \n",
    "    def plot_atoms(self):\n",
    "        \"\"\"plot all the atoms on one figure\"\"\"\n",
    "        fig = plt.figure()\n",
    "        \n",
    "        if self.dimension == 2:\n",
    "            fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "            ax = fig.add_subplot(111, aspect='equal', autoscale_on=False,\n",
    "                         xlim=self.coords_range, ylim=self.coords_range)\n",
    "            for atom in self.atoms:\n",
    "                ax.scatter(atom.coords[0],atom.coords[1], color = 'black')\n",
    "            \n",
    "        if self.dimension == 3:\n",
    "            ax = Axes3D(fig)\n",
    "            for atom in self.atoms:\n",
    "                ax.scatter(atom.coords[0], atom.coords[1],atom.coords[2])\n",
    "            ax.set_zlabel('Z', fontdict={'size': 15, 'color': 'black'})\n",
    "            ax.set_ylabel('Y', fontdict={'size': 15, 'color': 'black'})\n",
    "            ax.set_xlabel('X', fontdict={'size': 15, 'color': 'black'})\n",
    "    \n",
    "    def clear_collection(self):\n",
    "        \"\"\"erase all the coordinates\"\"\"\n",
    "        self.atoms = []\n",
    "        self.num = 0\n",
    "        self.dimension = 0\n",
    "        self.potential = 0\n",
    "        self.coords_range=(-5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_reshape(atom_collection_set):\n",
    "    \"\"\"reshape the collections, with output like [[g11,g12,g13],[g21,g22,g23],...] & [E1,E2,...]\"\"\"\n",
    "    g_factors = []\n",
    "    potentials = []\n",
    "    for collection in atom_collection_set:\n",
    "        for atom in collection.atoms:\n",
    "            #g_factors.append([atom.g1_value,atom.g2_value])\n",
    "            g_factors.append([atom.g1_value,atom.g2_value,atom.g3_value])\n",
    "            potentials.append(atom.individual_potential)\n",
    "            #print(atom.individual_potential)\n",
    "            #print(len(potentials))\n",
    "    return np.array(g_factors),np.array(potentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(params,energies,batch_size,start_index = 0):\n",
    "    return params[start_index:start_index + batch_size],energies[start_index:start_index + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1_g1 = tf.keras.layers.Dense(units=100)\n",
    "        self.dense1_g2 = tf.keras.layers.Dense(units=100)\n",
    "        self.dense1_g3 = tf.keras.layers.Dense(units=100)\n",
    "        self.dense2_g1 = tf.keras.layers.Dense(units=20,activation = tf.nn.tanh)\n",
    "        self.dense2_g2 = tf.keras.layers.Dense(units=20,activation = tf.nn.tanh)\n",
    "        self.dense2_g3 = tf.keras.layers.Dense(units=20,activation = tf.nn.tanh)\n",
    "        self.dense3_g1 = tf.keras.layers.Dense(units=1)\n",
    "        self.dense3_g2 = tf.keras.layers.Dense(units=1)\n",
    "        self.dense3_g3 = tf.keras.layers.Dense(units=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        g1 = tf.slice(inputs,[0,0],[-1,1])  #tf.slice(data,begin,size)\n",
    "        g1 = tf.reshape(g1,[-1,1])\n",
    "        g2 = tf.slice(inputs,[0,1],[-1,1])\n",
    "        g2 = tf.reshape(g2,[-1,1])\n",
    "        g3 = tf.slice(inputs,[0,2],[-1,1])\n",
    "        g3 = tf.reshape(g3,[-1,1])\n",
    "        g1 = self.dense1_g1(g1)\n",
    "        g1 = self.dense2_g1(g1)\n",
    "        g1 = self.dense3_g1(g1)\n",
    "        g2 = self.dense1_g2(g2)\n",
    "        g2 = self.dense2_g2(g2)\n",
    "        g2 = self.dense3_g2(g2)\n",
    "        #g3 = self.dense1_g3(g3)\n",
    "        #g3 = self.dense2_g3(g3)\n",
    "        #g3 = self.dense3_g3(g3)\n",
    "        #output = g1 + g2 + g3\n",
    "        output = g1 + g2\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超参数hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network params\n",
    "num_epoch = 1.0\n",
    "batch_size = 10\n",
    "rate = 0.00000001\n",
    "\n",
    "#configurations generation params\n",
    "atom_size = 50\n",
    "configurations = 400\n",
    "dim = 3\n",
    "\n",
    "#cross validation params\n",
    "train_ratio = 0.9\n",
    "test_ratio = 0.1\n",
    "split = 10\n",
    "seed = 1  #不要每次运行程序cv结果一样就seed = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 0\n",
    "train_collections = []\n",
    "r_ij = []\n",
    "          \n",
    "while config < configurations:\n",
    "    temp = AtomCollection()\n",
    "    temp.random_generate(size = atom_size,dimension = dim)\n",
    "    for atom in temp.atoms:\n",
    "        r_ij = temp.distance_cal(atom)\n",
    "        atom.individual_potential_cal(r_ij)\n",
    "        atom.g1_cal(r_ij)\n",
    "        atom.g2_cal(r_ij)\n",
    "        atom.g3_cal(r_ij)\n",
    "        r_ij = []\n",
    "    train_collections.append(temp)\n",
    "    config += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.57265367, -0.01792639, -0.59706773, -5.70994758, -0.73442609,\n",
       "       -0.55097378, -0.34194468, -0.20500102, -2.67012234, -0.00751078,\n",
       "       -0.04067032, -0.12556378, -0.00934528, -0.21865232, -0.22724583])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_factors,potentials = data_reshape(train_collections)\n",
    "#len(train_collections)\n",
    "#g_factors[:15]\n",
    "potentials[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练及测试评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=rate)\n",
    "num_batches = int(len(potentials)*train_ratio//batch_size*num_epoch)\n",
    "rs = ShuffleSplit(n_splits=split,test_size=test_ratio,\n",
    "                  train_size=train_ratio,random_state=seed)  #划分数据集为训练集和测试集\n",
    "times = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times 1\n",
      "batch 0: loss 5.49675989151001\n",
      "batch 100: loss 3.8996901512145996\n",
      "batch 200: loss 3.3908774852752686\n",
      "batch 300: loss 2574467.25\n",
      "batch 400: loss 1319174.875\n",
      "batch 500: loss 1.502172589302063\n",
      "batch 600: loss 5.0454230308532715\n",
      "batch 700: loss 12.886371612548828\n",
      "batch 800: loss 664905252864.0\n",
      "batch 900: loss 154.4141387939453\n",
      "batch 1000: loss 28925.044921875\n",
      "batch 1100: loss 2.557191101035315e+16\n",
      "batch 1200: loss 6.196959018707275\n",
      "batch 1300: loss 4.3756608963012695\n",
      "batch 1400: loss 2.7315046787261963\n",
      "batch 1500: loss 21.66996192932129\n",
      "batch 1600: loss 11.284544944763184\n",
      "batch 1700: loss 77180.7734375\n",
      "average_error -0.10943256786580177\n",
      "\n",
      "\n",
      "times 2\n",
      "batch 0: loss 272.8937072753906\n",
      "batch 100: loss 161775872.0\n",
      "batch 200: loss 1732871.75\n",
      "batch 300: loss 803.5100708007812\n",
      "batch 400: loss 7411123200.0\n",
      "batch 500: loss 1354637.625\n",
      "batch 600: loss 2678.078857421875\n",
      "batch 700: loss 32959.359375\n",
      "batch 800: loss 521048.28125\n",
      "batch 900: loss 32710.408203125\n",
      "batch 1000: loss 764.681884765625\n",
      "batch 1100: loss 11.343388557434082\n",
      "batch 1200: loss 12.313645362854004\n",
      "batch 1300: loss 4.3108906745910645\n",
      "batch 1400: loss 227310.375\n",
      "batch 1500: loss 585.6160888671875\n",
      "batch 1600: loss 17.062549591064453\n",
      "batch 1700: loss 19625272.0\n",
      "average_error -0.14636600194194568\n",
      "\n",
      "\n",
      "times 3\n",
      "batch 0: loss 741732672.0\n",
      "batch 100: loss 7.86461877822876\n",
      "batch 200: loss 11685.96484375\n",
      "batch 300: loss 5.29562520980835\n",
      "batch 400: loss 1339.4566650390625\n",
      "batch 500: loss 1.517192006111145\n",
      "batch 600: loss 5712403.5\n",
      "batch 700: loss 11774978.0\n",
      "batch 800: loss 9.630814552307129\n",
      "batch 900: loss 3559945928704.0\n",
      "batch 1000: loss 27364.53515625\n",
      "batch 1100: loss 5.206945419311523\n",
      "batch 1200: loss 235399.75\n",
      "batch 1300: loss 1564518144.0\n",
      "batch 1400: loss 1496.0152587890625\n",
      "batch 1500: loss 334.4019775390625\n",
      "batch 1600: loss 27986489344.0\n",
      "batch 1700: loss 22.08606719970703\n",
      "average_error -0.17652806811332913\n",
      "\n",
      "\n",
      "times 4\n",
      "batch 0: loss 19657244.0\n",
      "batch 100: loss 258576144.0\n",
      "batch 200: loss 320174.0\n",
      "batch 300: loss 75817877504.0\n",
      "batch 400: loss 2204366.5\n",
      "batch 500: loss 718.189208984375\n",
      "batch 600: loss 88.67768096923828\n",
      "batch 700: loss 15.242331504821777\n",
      "batch 800: loss 10883.0625\n",
      "batch 900: loss 694.4796752929688\n",
      "batch 1000: loss 3878336.25\n",
      "batch 1100: loss 6.139233589172363\n",
      "batch 1200: loss 6.918439865112305\n",
      "batch 1300: loss 1380835968.0\n",
      "batch 1400: loss 165118709071872.0\n",
      "batch 1500: loss 665952976896.0\n",
      "batch 1600: loss 551.1513061523438\n",
      "batch 1700: loss 10.963672637939453\n",
      "average_error -0.024919953373773417\n",
      "\n",
      "\n",
      "times 5\n",
      "batch 0: loss 3.671360378450739e+17\n",
      "batch 100: loss 9.456241607666016\n",
      "batch 200: loss 2222.19775390625\n",
      "batch 300: loss 549064.0\n",
      "batch 400: loss 114632240922624.0\n",
      "batch 500: loss 578494592.0\n",
      "batch 600: loss 1765902.375\n",
      "batch 700: loss 8282.947265625\n",
      "batch 800: loss 65390.796875\n",
      "batch 900: loss 12343.0478515625\n",
      "batch 1000: loss 11873.0537109375\n",
      "batch 1100: loss 2.44079852104187\n",
      "batch 1200: loss 6.73213005065918\n",
      "batch 1300: loss 3.5194945335388184\n",
      "batch 1400: loss 3.3596959114074707\n",
      "batch 1500: loss 7.238758087158203\n",
      "batch 1600: loss 1318180.5\n",
      "batch 1700: loss 259186.046875\n",
      "average_error -0.16666640415456352\n",
      "\n",
      "\n",
      "times 6\n",
      "batch 0: loss 112828.703125\n",
      "batch 100: loss 4.685990333557129\n",
      "batch 200: loss 27678.17578125\n",
      "batch 300: loss 3.5902562141418457\n",
      "batch 400: loss 46.812103271484375\n",
      "batch 500: loss 0.42263442277908325\n",
      "batch 600: loss 5.830082416534424\n",
      "batch 700: loss 1319807104.0\n",
      "batch 800: loss 3203657984.0\n",
      "batch 900: loss 4665.001953125\n",
      "batch 1000: loss 383161761792.0\n",
      "batch 1100: loss 8514261.0\n",
      "batch 1200: loss 143.0652313232422\n",
      "batch 1300: loss 23.312114715576172\n",
      "batch 1400: loss 3305.396484375\n",
      "batch 1500: loss 3.568721294403076\n",
      "batch 1600: loss 62194112.0\n",
      "batch 1700: loss 2875.297119140625\n",
      "average_error -0.14219908779740895\n",
      "\n",
      "\n",
      "times 7\n",
      "batch 0: loss 12.555045127868652\n",
      "batch 100: loss 741679872.0\n",
      "batch 200: loss 236254953472.0\n",
      "batch 300: loss 13.145190238952637\n",
      "batch 400: loss 533204736.0\n",
      "batch 500: loss 20.229347229003906\n",
      "batch 600: loss 156610432.0\n",
      "batch 700: loss 3.671389927825736e+17\n",
      "batch 800: loss 40.79890060424805\n",
      "batch 900: loss 236254904320.0\n",
      "batch 1000: loss 3.2996697425842285\n",
      "batch 1100: loss 6.982061386108398\n",
      "batch 1200: loss 27986489344.0\n",
      "batch 1300: loss 0.1079459935426712\n",
      "batch 1400: loss 1.2400658130645752\n",
      "batch 1500: loss 2924793.75\n",
      "batch 1600: loss 14869431296.0\n",
      "batch 1700: loss 610768.8125\n",
      "average_error -0.1653712909837264\n",
      "\n",
      "\n",
      "times 8\n",
      "batch 0: loss 2.7812745571136475\n",
      "batch 100: loss 957016.375\n",
      "batch 200: loss 14873.6923828125\n",
      "batch 300: loss 315429152.0\n",
      "batch 400: loss 28294.3828125\n",
      "batch 500: loss 5.96271276473999\n",
      "batch 600: loss 193426640.0\n",
      "batch 700: loss 171460.515625\n",
      "batch 800: loss 20.019412994384766\n",
      "batch 900: loss 19.978544235229492\n",
      "batch 1000: loss 8.547507286071777\n",
      "batch 1100: loss 5.350766181945801\n",
      "batch 1200: loss 76884.734375\n",
      "batch 1300: loss 2.7595434188842773\n",
      "batch 1400: loss 20317336.0\n",
      "batch 1500: loss 3073.747314453125\n",
      "batch 1600: loss 299081920.0\n",
      "batch 1700: loss 14.95017147064209\n",
      "average_error -0.10394258550256857\n",
      "\n",
      "\n",
      "times 9\n",
      "batch 0: loss 111649512.0\n",
      "batch 100: loss 116012.59375\n",
      "batch 200: loss 8.334060668945312\n",
      "batch 300: loss 3.2946548461914062\n",
      "batch 400: loss 37625.08984375\n",
      "batch 500: loss 1095810688.0\n",
      "batch 600: loss 8.657697677612305\n",
      "batch 700: loss 1.3222535848617554\n",
      "batch 800: loss 8.709848403930664\n",
      "batch 900: loss 47.74119186401367\n",
      "batch 1000: loss 5.281989435326464e+16\n",
      "batch 1100: loss 17909188.0\n",
      "batch 1200: loss 8.009289741516113\n",
      "batch 1300: loss 24.917072296142578\n",
      "batch 1400: loss 3.1807875633239746\n",
      "batch 1500: loss 9.196086883544922\n",
      "batch 1600: loss 27926.462890625\n",
      "batch 1700: loss 745695616.0\n",
      "average_error -0.08327186706120976\n",
      "\n",
      "\n",
      "times 10\n",
      "batch 0: loss 1103788900352.0\n",
      "batch 100: loss 505.1457214355469\n",
      "batch 200: loss 397077.71875\n",
      "batch 300: loss 2.8900318145751953\n",
      "batch 400: loss 947998.875\n",
      "batch 500: loss 14869431296.0\n",
      "batch 600: loss 7.189583778381348\n",
      "batch 700: loss 4246.2998046875\n",
      "batch 800: loss 3.7535572052001953\n",
      "batch 900: loss 6748.73876953125\n",
      "batch 1000: loss 1324192384.0\n",
      "batch 1100: loss 5335215616.0\n",
      "batch 1200: loss 2432095488.0\n",
      "batch 1300: loss 91136672.0\n",
      "batch 1400: loss 9.294597625732422\n",
      "batch 1500: loss 2.2272796952980517e+23\n",
      "batch 1600: loss 2743836.0\n",
      "batch 1700: loss 379211.875\n",
      "average_error -0.1540530741309716\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for train_index,test_index in rs.split(potentials):\n",
    "    print(\"times {}\".format(times))\n",
    "    for batch_index in range(num_batches):\n",
    "        X_train,y_train = get_batch(g_factors[train_index],potentials[train_index],batch_size,\n",
    "                                    batch_size * batch_index)  #X: (batch_size,num_of_factors), y: (batch_size,)\n",
    "        X_train = tf.convert_to_tensor(X_train,dtype = 'float32')\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_train)\n",
    "            y_temp = tf.convert_to_tensor(y_train.reshape(-1,1),dtype = 'float32')\n",
    "            loss = 0.5 * tf.reduce_sum(tf.square(y_pred - y_temp))\n",
    "            if batch_index % 100 == 0:\n",
    "                print(\"batch {}: loss {}\".format(batch_index,loss.numpy()))\n",
    "        grad = tape.gradient(loss,model.variables)  #model.variables直接调用模型变量\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grad,model.variables))\n",
    "    \n",
    "    X_test,y_test = g_factors[test_index],potentials[test_index]\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    error = (y_pred_test - y_test)/y_test\n",
    "    print(\"average_error {}\".format(np.mean(error)))\n",
    "\n",
    "    times += 1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
